<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ridge Regression | Daniel Namgoong </title> <meta name="author" content="Daniel Namgoong"> <meta name="description" content="Linear regression"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dnamgoong.github.io/blog/2025/ridge/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Daniel</span> Namgoong </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Ridge Regression</h1> <p class="post-meta"> Created on August 23, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>In this post, we implement the Ridge regression estimator described in <a href="https://arxiv.org/abs/2306.15063" rel="external nofollow noopener" target="_blank">the paper</a></p> <ul> <li>Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. <ul> <li> <a href="https://simons.berkeley.edu/talks/surya-ganguli-stanford-university-2023-08-18" rel="external nofollow noopener" target="_blank">Large Language Models and Transformers Workshop</a>, Simon Institute for the Theory of Computing, August 2023.</li> </ul> </li> </ul> <h2 id="data-distribution">Data distribution</h2> <p>For each batch element, we generate the data as follows \begin{equation} y = {\bf{w}}^T {\bf{x}} + \epsilon \end{equation}</p> <ul> <li>${\bf{w}} \in \mathbb{R}^D$, ${\bf{x}} \in \mathbb{R}^D$.</li> <li>${\bf{x}} \sim \mathcal{N}({\bf 0}, {\bf{I}}_D)$, which means $E[\bf{x}]=0$ and $E[{\bf xx}^T]={\bf I}_D$.</li> <li>“Task vector”: ${\bf w} \sim \mathcal{T}_{true} = \mathcal{N}({\bf 0}, {\bf I}_D)$, and the test data is generated according to this model.</li> <li>observation noise: $\epsilon \sim \mathcal{N}(0, \sigma^2)$, which means $E[\epsilon]=0$, and $E[\epsilon^2]= \sigma^2$</li> <li>$K$ = the number of examples $({\bf x}, y)$ generated for the prompt in each batch element. <ul> <li>The last one will become a query.</li> </ul> </li> </ul> <p>But, for a pre-training dataset, we sample {${\bf w}^{(1)}, {\bf w}^{(2)}, \cdots, {\bf w}^{(M)} $} once from $\mathcal{T}_{true}$</p> <ul> <li>Each batch element is generated using one of the $M$ tasks {${\bf w}^{(1)}, {\bf w}^{(2)}, \cdots, {\bf w}^{(M)}$}.</li> </ul> <p>In <a href="https://arxiv.org/abs/2306.15063" rel="external nofollow noopener" target="_blank">Raventos et al.</a>, the distrubtion of ${\bf w}$ in the pretraining data is denoted by \begin{equation} \mathcal{T}_{pretrain} = \mathcal{U} ({\bf w}^{(1)}, {\bf w}^{(2)}, \cdots, {\bf w}^{(M)} ) \end{equation}</p> <p>For each batch element, a “prompt” consists of a bunch of $K-1$ examples $({\bf x}, y)$ generated from one $\bf{w}$, and a query ${\bf{x}}_{query}$.</p> <p>Given a prompt, we want to predict what $\bf{w}$ would be, denoted by $\hat{\bf{w}}$, and predict the output $\hat{y}$ corresponding to the query ${\bf{x}}_{query}$, i.e.</p> <p>\begin{equation} \hat{y} = \hat{\bf{w}}^T {\bf{x}}_{query}. \end{equation}</p> <p>To quantify the accuracy of the predictions, the MSE is used; the average squared difference between the true $y$ and its prediction $\hat{y}$.</p> <h2 id="ridge-regression-estimator">Ridge regression estimator</h2> <p>The Ridge regression estimator ${\bf w}_{ridge}$ is derived assuming that</p> <ol> <li>we <strong>know</strong> how $y$ is generated from ${\bf x}$, i.e. $y={\bf w}^T{\bf x} + \epsilon$,</li> <li>we <strong>do not know</strong> ${\bf w}$ or $\epsilon$.</li> <li>But, we <strong>know</strong> the stastical distribution that generates the data $({\bf x},y)$ that we observe <ul> <li>${\bf w} \sim \mathcal{T}_{true}=\mathcal{N}({\bf 0}, {\bf I}_D)$</li> <li>$\epsilon \sim \mathcal{N}(0, \sigma^2)$.</li> </ul> </li> </ol> <p>So, the performance of ridge regression is what we would achieve, had we known that ${\bf w} \sim \mathcal{T}_{true}=\mathcal{N}({\bf 0}, {\bf I}_D)$ and $\epsilon \sim \mathcal{N}(0, \sigma^2)$.</p> <p>Since we generate the test data according to ${\bf w} \sim \mathcal{T}_{true}=\mathcal{N}({\bf 0}, {\bf I}_D)$ and $\epsilon \sim \mathcal{N}(0, \sigma^2)$, the ridge regressor is the best predictor in terms of minimizing the MSE.</p> <p>We will evaluate the accuracy of predicted $y_{K}$ in response to the ${\bf x}_{query}$, given the in-context learning examples in the prompt</p> <p>\(\{ ({\bf x}_1, y_1), ({\bf x}_2, y_2), \cdots, ({\bf x}_{K-1}, y_{K-1}) \}\).</p> <h3 id="implementation">Implementation</h3> <p>For each batch element, we want to compute the equation (3) of <a href="https://arxiv.org/abs/2306.15063" rel="external nofollow noopener" target="_blank">Raventos et al.</a>, \begin{equation} {\bf w}_{ridge} = (X^T X + \sigma^2 {\bf I}_D)^{-1} X^T {\bf y} \end{equation}</p> <ul> <li>$X$ is a $(K-1) \times D$ matrix:</li> </ul> \[X = \left[ \begin{matrix} &amp;&amp; {\bf x}_1^T &amp;&amp;\\ &amp;&amp; {\bf x}_2^T &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf x}_{K-1}^T &amp;&amp; \end{matrix} \right]\] <ul> <li>${\bf y}$ is a $(K-1) \times 1$ vector:</li> </ul> \[{\bf y} = \left[ \begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_{K-1} \end{matrix} \right]\] <p>In the PyTorch code, we use 3D-tensors of the following dimensions</p> <ul> <li>$X$ = (batch_size, K-1, D)</li> <li>${\bf y}$ = (batch_size, K-1, 1)</li> </ul> <p>Now, $X$ denotes a 3D-tensor, and $X_{b,:,:}$ a matrix corresponding to the batch element $b$ of the 3D tensor.</p> <p>We will write some equations that help us implement the ridge estimator using einsum function.</p> <p>To compute the Gram matrix for $X_{b,:,:}$, where $b$ is the batch element index,</p> \[[X_{b,:,:}^TX_{b,:,:}]_{i,k} = \sum_j (X_{b,:,:}^T)_{i,j} X_{b,j,k}\] \[C_{b, :, :} = X_{b, :, :}^T X_{b, :, :} + \sigma^2 {\bf I}_D\] <p>To compute $(X^T X + \sigma^2 {\bf I}_D)^{-1} X^T$ for each batch element,</p> \[[C_{b,:,:}^{-1} X_{b,:,:}^T]_{i,k} = \sum_j [C_{b,:,:}^{-1}]_{i,j} [X_{b,:,:}^T]_{j,k}\] <p>Finally, to compute the ${\bf w}_{ridge}$ for each batch element,</p> \[w_{ridge}[b, i] = [C_{b,:,:}^{-1} X_{b,:,:}^T {\bf y}_{b,:, 0}]_{i} = \sum_{j}[C_{b,:,:}^{-1} X_{b,:,:}^T]_{i,j} {\bf y}_{b,j, 0}\] <p>Predict $y$ using the ridge regressor ${\bf w}_{ridge}$ for each batch element:</p> \[\hat{y} = {\bf w}_{ridge}^T {\bf x}_{query}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
 x = (batch_size, K, D)
 y = (batch_size, K, 1)
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="nf">ridge</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">):</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="c1"># batch_size, K-1, D
</span>    <span class="n">I</span> <span class="o">=</span> <span class="n">sigma_sq</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">D</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
    <span class="n">Cxx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">permute</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">I</span><span class="p">[</span><span class="bp">None</span><span class="p">,:,:]</span> <span class="c1"># X'*X
</span>    <span class="n">Cxx_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Cxx</span><span class="p">)</span> <span class="c1"># batch_size, D, D
</span>
    <span class="n">Cxx_inv_X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">Cxx_inv</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">permute</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># batch_size, D, K-1
</span>    
    <span class="c1"># w_ridge = (X'X + sigma_sq*I)^-1 X'y
</span>    <span class="n">w_ridge</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bj -&gt; bi</span><span class="sh">'</span><span class="p">,</span> <span class="n">Cxx_inv_X</span><span class="p">,</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># batch_size, D
</span>    <span class="n">query</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span><span class="c1"># batch_size, D
</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span> <span class="n">w_ridge</span><span class="o">*</span><span class="n">query</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># batch_size, 1
</span>
    <span class="k">return</span> <span class="n">yhat</span>

</code></pre></div></div> <h2 id="jupyter-notebook">Jupyter Notebook</h2> <p>Some portion of a <a href="https://github.com/pytorch/examples/blob/main/mnist/main.py" rel="external nofollow noopener" target="_blank">Basic MNIST Example code</a> is used.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/ridge_post.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <h3 id="observations">Observations</h3> <p>We observe that when $K=2$, the MSE is larger than when $K=16$. This is because when $K-1$ &lt; $D$, we have fewer equations than the number of unknowns, which is the same as the dimensionality of ${\bf w}$.</p> \[y_1 = {\bf w}^T {\bf x}_1 + \epsilon_1\] \[y_2 = {\bf w}^T {\bf x}_2 + \epsilon_2\] \[\ \ \ \ \ \ \ \vdots\] \[y_{K-1} = {\bf w}^T {\bf x}_{K-1} + \epsilon_{K-1}\] </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Daniel Namgoong. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>