<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://dnamgoong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dnamgoong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-06T04:45:47+00:00</updated><id>https://dnamgoong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Ridge Regression</title><link href="https://dnamgoong.github.io/blog/2025/ridge/" rel="alternate" type="text/html" title="Ridge Regression"/><published>2025-08-23T00:00:00+00:00</published><updated>2025-08-23T00:00:00+00:00</updated><id>https://dnamgoong.github.io/blog/2025/ridge</id><content type="html" xml:base="https://dnamgoong.github.io/blog/2025/ridge/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we implement the Ridge regression estimator described in <a href="https://arxiv.org/abs/2306.15063">the paper</a></p> <ul> <li>Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. <ul> <li><a href="https://simons.berkeley.edu/talks/surya-ganguli-stanford-university-2023-08-18">Large Language Models and Transformers Workshop</a>, Simon Institute for the Theory of Computing, August 2023.</li> </ul> </li> </ul> <h2 id="data-distribution">Data distribution</h2> <p>For each batch element, we generate the data as follows \begin{equation} y = {\bf{w}}^T {\bf{x}} + \epsilon \end{equation}</p> <ul> <li>${\bf{w}} \in \mathbb{R}^D$, ${\bf{x}} \in \mathbb{R}^D$.</li> <li>${\bf{x}} \sim \mathcal{N}({\bf 0}, {\bf{I}}_D)$, which means $E[\bf{x}]=0$ and $E[{\bf xx}^T]={\bf I}_D$.</li> <li>“Task vector”: ${\bf w} \sim \mathcal{T}_{true} = \mathcal{N}({\bf 0}, {\bf I}_D)$, and the test data is generated according to this model.</li> <li>observation noise: $\epsilon \sim \mathcal{N}(0, \sigma^2)$, which means $E[\epsilon]=0$, and $E[\epsilon^2]= \sigma^2$</li> <li>$K$ = the number of examples $({\bf x}, y)$ generated for the prompt in each batch element. <ul> <li>The last one will become a query.</li> </ul> </li> </ul> <p>But, for a pre-training dataset, we sample {${\bf w}^{(1)}, {\bf w}^{(2)}, \cdots, {\bf w}^{(M)} $} once from $\mathcal{T}_{true}$</p> <ul> <li>Each batch element is generated using one of the $M$ tasks {${\bf w}^{(1)}, {\bf w}^{(2)}, \cdots, {\bf w}^{(M)}$}.</li> </ul> <p>In <a href="https://arxiv.org/abs/2306.15063">Raventos et al.</a>, the distrubtion of ${\bf w}$ in the pretraining data is denoted by \begin{equation} \mathcal{T}_{pretrain} = \mathcal{U} ({\bf w}^{(1)}, {\bf w}^{(2)}, \cdots, {\bf w}^{(M)} ) \end{equation}</p> <p>For each batch element, a “prompt” consists of a bunch of $K-1$ examples $({\bf x}, y)$ generated from one $\bf{w}$, and a query ${\bf{x}}_{query}$.</p> <p>Given a prompt, we want to predict what $\bf{w}$ would be, denoted by $\hat{\bf{w}}$, and predict the output $\hat{y}$ corresponding to the query ${\bf{x}}_{query}$, i.e.</p> <p>\begin{equation} \hat{y} = \hat{\bf{w}}^T {\bf{x}}_{query}. \end{equation}</p> <p>To quantify the accuracy of the predictions, the MSE is used; the average squared difference between the true $y$ and its prediction $\hat{y}$.</p> <h2 id="ridge-regression-estimator">Ridge regression estimator</h2> <p>The Ridge regression estimator ${\bf w}_{ridge}$ is derived assuming that</p> <ol> <li>we <strong>know</strong> how $y$ is generated from ${\bf x}$, i.e. $y={\bf w}^T{\bf x} + \epsilon$,</li> <li>we <strong>do not know</strong> ${\bf w}$ or $\epsilon$.</li> <li>But, we <strong>know</strong> the stastical distribution that generates the data $({\bf x},y)$ that we observe <ul> <li>${\bf w} \sim \mathcal{T}_{true}=\mathcal{N}({\bf 0}, {\bf I}_D)$</li> <li>$\epsilon \sim \mathcal{N}(0, \sigma^2)$.</li> </ul> </li> </ol> <p>So, the performance of ridge regression is what we would achieve, had we known that ${\bf w} \sim \mathcal{T}_{true}=\mathcal{N}({\bf 0}, {\bf I}_D)$ and $\epsilon \sim \mathcal{N}(0, \sigma^2)$.</p> <p>Since we generate the test data according to ${\bf w} \sim \mathcal{T}_{true}=\mathcal{N}({\bf 0}, {\bf I}_D)$ and $\epsilon \sim \mathcal{N}(0, \sigma^2)$, the ridge regressor is the best predictor in terms of minimizing the MSE.</p> <p>We will evaluate the accuracy of predicted $y_{K}$ in response to the ${\bf x}_{query}$, given the in-context learning examples in the prompt</p> <p>\(\{ ({\bf x}_1, y_1), ({\bf x}_2, y_2), \cdots, ({\bf x}_{K-1}, y_{K-1}) \}\).</p> <h3 id="implementation">Implementation</h3> <p>For each batch element, we want to compute the equation (3) of <a href="https://arxiv.org/abs/2306.15063">Raventos et al.</a>, \begin{equation} {\bf w}_{ridge} = (X^T X + \sigma^2 {\bf I}_D)^{-1} X^T {\bf y} \end{equation}</p> <ul> <li>$X$ is a $(K-1) \times D$ matrix:</li> </ul> \[X = \left[ \begin{matrix} &amp;&amp; {\bf x}_1^T &amp;&amp;\\ &amp;&amp; {\bf x}_2^T &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf x}_{K-1}^T &amp;&amp; \end{matrix} \right]\] <ul> <li>${\bf y}$ is a $(K-1) \times 1$ vector:</li> </ul> \[{\bf y} = \left[ \begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_{K-1} \end{matrix} \right]\] <p>In the PyTorch code, we use 3D-tensors of the following dimensions</p> <ul> <li>$X$ = (batch_size, K-1, D)</li> <li>${\bf y}$ = (batch_size, K-1, 1)</li> </ul> <p>Now, $X$ denotes a 3D-tensor, and $X_{b,:,:}$ a matrix corresponding to the batch element $b$ of the 3D tensor.</p> <p>We will write some equations that help us implement the ridge estimator using einsum function.</p> <p>To compute the Gram matrix for $X_{b,:,:}$, where $b$ is the batch element index,</p> \[[X_{b,:,:}^TX_{b,:,:}]_{i,k} = \sum_j (X_{b,:,:}^T)_{i,j} X_{b,j,k}\] \[C_{b, :, :} = X_{b, :, :}^T X_{b, :, :} + \sigma^2 {\bf I}_D\] <p>To compute $(X^T X + \sigma^2 {\bf I}_D)^{-1} X^T$ for each batch element,</p> \[[C_{b,:,:}^{-1} X_{b,:,:}^T]_{i,k} = \sum_j [C_{b,:,:}^{-1}]_{i,j} [X_{b,:,:}^T]_{j,k}\] <p>Finally, to compute the ${\bf w}_{ridge}$ for each batch element,</p> \[w_{ridge}[b, i] = [C_{b,:,:}^{-1} X_{b,:,:}^T {\bf y}_{b,:, 0}]_{i} = \sum_{j}[C_{b,:,:}^{-1} X_{b,:,:}^T]_{i,j} {\bf y}_{b,j, 0}\] <p>Predict $y$ using the ridge regressor ${\bf w}_{ridge}$ for each batch element:</p> \[\hat{y} = {\bf w}_{ridge}^T {\bf x}_{query}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
 x = (batch_size, K, D)
 y = (batch_size, K, 1)
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="nf">ridge</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">):</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="c1"># batch_size, K-1, D
</span>    <span class="n">I</span> <span class="o">=</span> <span class="n">sigma_sq</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">D</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
    <span class="n">Cxx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">permute</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">I</span><span class="p">[</span><span class="bp">None</span><span class="p">,:,:]</span> <span class="c1"># X'*X
</span>    <span class="n">Cxx_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Cxx</span><span class="p">)</span> <span class="c1"># batch_size, D, D
</span>
    <span class="n">Cxx_inv_X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">Cxx_inv</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">permute</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># batch_size, D, K-1
</span>    
    <span class="c1"># w_ridge = (X'X + sigma_sq*I)^-1 X'y
</span>    <span class="n">w_ridge</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bj -&gt; bi</span><span class="sh">'</span><span class="p">,</span> <span class="n">Cxx_inv_X</span><span class="p">,</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># batch_size, D
</span>    <span class="n">query</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span><span class="c1"># batch_size, D
</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span> <span class="n">w_ridge</span><span class="o">*</span><span class="n">query</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># batch_size, 1
</span>
    <span class="k">return</span> <span class="n">yhat</span>

</code></pre></div></div> <h2 id="jupyter-notebook">Jupyter Notebook</h2> <p>Some portion of a <a href="https://github.com/pytorch/examples/blob/main/mnist/main.py">Basic MNIST Example code</a> is used.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/ridge_post.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <h3 id="observations">Observations</h3> <p>We observe that when $K=2$, the MSE is larger than when $K=16$. This is because when $K-1$ &lt; $D$, we have fewer equations than the number of unknowns, which is the same as the dimensionality of ${\bf w}$.</p> \[y_1 = {\bf w}^T {\bf x}_1 + \epsilon_1\] \[y_2 = {\bf w}^T {\bf x}_2 + \epsilon_2\] \[\ \ \ \ \ \ \ \vdots\] \[y_{K-1} = {\bf w}^T {\bf x}_{K-1} + \epsilon_{K-1}\]]]></content><author><name></name></author><category term="code"/><category term="math"/><summary type="html"><![CDATA[Linear regression]]></summary></entry><entry><title type="html">Einsum</title><link href="https://dnamgoong.github.io/blog/2025/einsum/" rel="alternate" type="text/html" title="Einsum"/><published>2025-08-12T00:00:00+00:00</published><updated>2025-08-12T00:00:00+00:00</updated><id>https://dnamgoong.github.io/blog/2025/einsum</id><content type="html" xml:base="https://dnamgoong.github.io/blog/2025/einsum/"><![CDATA[<p><a href="https://docs.pytorch.org/docs/stable/generated/torch.einsum.html">Einsum</a> can make multi-dimensional linear algebraic array operations simple, with unified syntax. It is simpler to describe it using math than words. I will give a few examples of its usage in PyTorch.</p> <p><strong>Notation</strong>:</p> <ol> <li>The entry in row $i$, column $j$ of matrix $A$ is denoted by $A_{i,j}$.</li> <li>An entry in a three diemensional tensor $X$ will be similarly denoted $X_{b, i, j}$.</li> <li>$X_{b, :, :}$ is a matrix corresponding to the batch element $b$ of the 3D tensor $X$.</li> </ol> <p>We have matrices $X$ and $Y$, and want to implement $Z=XY$, where</p> <p>\begin{equation} Z_{i,k} = \sum_{j} X_{i,j} Y_{j,k} \end{equation}</p> <p>In PyTorch, this can be implemented as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ij, jk -&gt; ik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</code></pre></div></div> <p>Now, suppose that $X$ and $Y$ represent 3-D tensors, where the first dimension represents a batch dimension, and we want to perform matrix multiplication over the last two dimenensions, for each batch element. \begin{equation} Z_{b, i,k} = \sum_{j} X_{b, i,j} Y_{b, j,k} \end{equation}</p> <p>In PyTorch, this can be implemented as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</code></pre></div></div> <p>In the final example, we compute the Gram matrix $G_{b,:,:}=X_{b,:,:}^T X_{b,:,:}$ over the last two dimensions of 3-D array $X$. For each batch element $b$, we have a matrix $X_{b,:,:}$, and compute its Gram matrix \begin{equation} G_{b,i,j} = \sum_j X_{b,j,i} X_{b,j,k} \end{equation}</p> <p>So, we can implement either</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bji, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <p>or</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/einsum.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="code"/><category term="math"/><summary type="html"><![CDATA[Einsum tutorial]]></summary></entry><entry><title type="html">Vision Transformer</title><link href="https://dnamgoong.github.io/blog/2025/vision-transformer/" rel="alternate" type="text/html" title="Vision Transformer"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://dnamgoong.github.io/blog/2025/vision-transformer</id><content type="html" xml:base="https://dnamgoong.github.io/blog/2025/vision-transformer/"><![CDATA[<p>These are my study notes on <a href="https://arxiv.org/abs/2010.11929">Vision Transformer</a>:</p> <ul> <li>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</li> </ul> <p>Very nice visualization of Fig.1 of <a href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al.</a> by <a href="https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif">Phil Wang (lucidrains)</a>:</p> <center width="100%"><img src="/assets/img/vit/vit.gif" width="500px"/></center> <h3 id="patchification">Patchification</h3> <ul> <li>An image ${\bf x}$ of shape <code class="language-plaintext highlighter-rouge">(H, W, 3)</code> is split into fixed sized patches, where each patch has shape <code class="language-plaintext highlighter-rouge">(P, P, 3)</code>. <ul> <li>We have 9 patches in the figure.</li> <li>$P$ can be 16 for “16x16 words”.</li> </ul> </li> <li>Each patch is flattened to a row vector ${\bf x}_p^i$ of length $3P^2$, for $i=1,2, \cdots, 9$.</li> <li>Linearly embed each of ${\bf x}_p^i$, for $i=1,2, \cdots, 9$, i.e. $\textcolor{red}{\bf x}_p^i \textcolor{red}{\bf E}$. <ul> <li>$\textcolor{red}{\bf E}$ is a $3P^2 \times D$ matrix.</li> </ul> </li> <li>Prepend a learnable embedding ${\bf x}_{class}$ (“classification token”) to the sequence of embedded patches to obtain a $10 \times D$ matrix</li> </ul> \[\left[ \begin{matrix} \textcolor{red}{\bf x}_{class} \\ \textcolor{red}{\bf x}_p^1 \textcolor{red}{\bf E} \\ \textcolor{red}{\bf x}_p^2 \textcolor{red}{\bf E} \\ \vdots \\ \textcolor{red}{\bf x}_p^9 \textcolor{red}{\bf E} \\ \end{matrix} \right]\] <ul> <li>Add position embeddings ${\bf E}_{pos}$ ( Eq (1) in <a href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al.</a> ):</li> </ul> \[{\bf z}_0 = \left[ \begin{matrix} \textcolor{red}{\bf x}_{class} \\ \textcolor{red}{\bf x}_p^1 \textcolor{red}{\bf E} \\ \textcolor{red}{\bf x}_p^2 \textcolor{red}{\bf E} \\ \vdots \\ \textcolor{red}{\bf x}_p^9 \textcolor{red}{\bf E} \\ \end{matrix} \right] + \textcolor{purple}{\bf E}_{pos} \ \in \mathbb{R}^{10 \times D}\] <p>The learnable embedding and the position embedding are implemented in PyTorch using <code class="language-plaintext highlighter-rouge">nn.Parameter</code>.</p> <p>[<a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py">Phill Wang’s implementation</a>]:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># x_class
</span><span class="n">self</span><span class="p">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<span class="c1"># E_pos
</span><span class="n">self</span><span class="p">.</span><span class="n">pos_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
</code></pre></div></div> <p>The 10 row vectors or ``embedded patches’’ in ${\bf z}_0$ are fed to a standard transformer encoder, which we will describe next.</p> <h3 id="transformer-encoder">Transformer Encoder</h3> <p>A transformer encoder is formed by stacking $L$ transformer encoder layers.</p> <center width="100%"><img src="/assets/img/vit/transformer_encoder.png" width="200px"/></center> <p>Suppose the $l$-th layer is denoted by a function $f_{\theta^{(l)}}$, where $\theta^{(l)}$ represents the neural network paramters of the $l$-th layer.</p> <p>The 10 embedded patches in ${\bf z}_0$ are fed to the first transformer encoder layer \(f_{\theta^{(1)}}\).</p> \[{\bf z}_1 = f_{\theta^{(1)}} ({\bf z}_0)\] \[{\bf z}_2 = f_{\theta^{(2)}} ({\bf z}_1)\] \[\ \ \ \ \ \ \ \vdots\] \[{\bf z}_L = f_{\theta^{(L)}} ({\bf z}_{L-1})\] <p>Note that ${\bf z}_l \in \mathbb{R}^{10 \times D}$, for $l=0, 1,2, \cdots, L$,</p> \[{\bf z}_l = \left[ \begin{matrix} &amp;&amp; \textcolor{red}{\bf z}_l^0 &amp;&amp;\\ &amp;&amp; \textcolor{green}{\bf z}_l^1 &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf z}_{l}^{9} &amp;&amp; \end{matrix} \right]\] <p>where ${\bf z}_l^i$ is a $1 \times D$ vector.</p> <p>Notice how the transformer transforms the input ${\bf z}_0$ to the output ${\bf z}_L$:</p> \[\left[ \begin{matrix} &amp;&amp; \textcolor{red}{\bf z}_0^0 &amp;&amp; \\ &amp;&amp; \textcolor{green}{\bf z}_0^1 &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf z}_{0}^{9} &amp;&amp; \end{matrix} \right] \rightarrow \left[ \begin{matrix} &amp;&amp; \textcolor{red}{\bf z}_1^0&amp;&amp;\\ &amp;&amp; \textcolor{green}{\bf z}_1^1 &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf z}_{1}^{9} &amp;&amp; \end{matrix} \right] \rightarrow \left[ \begin{matrix} &amp;&amp; \textcolor{red}{\bf z}_2^0 &amp;&amp;\\ &amp;&amp; \textcolor{green}{\bf z}_2^1 &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf z}_{2}^{9} &amp;&amp; \end{matrix} \right] \rightarrow \cdots \rightarrow \left[ \begin{matrix} &amp;&amp; \textcolor{red}{\bf z}_{L-1}^0 &amp;&amp;\\ &amp;&amp; \textcolor{green}{\bf z}_{L-1}^1 &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf z}_{L-1}^{9} &amp;&amp; \end{matrix} \right] \rightarrow \left[ \begin{matrix} &amp;&amp; \textcolor{red}{\bf z}_L^0 &amp;&amp;\\ &amp;&amp; \textcolor{green}{\bf z}_L^1 &amp;&amp;\\ &amp;&amp; \vdots &amp;&amp;\\ &amp;&amp; {\bf z}_{L}^{9} &amp;&amp; \end{matrix} \right]\] <p>Using the self-attention layer in $f_{\theta^{(l)}}$, the patch embeddings in the matrix ${\bf z}_{l-1}$ communicate with each other and become the patch embeddings in ${\bf z}_l$.</p> <p>For example, ${\bf z}_{l-1}^0$ talks with \({\bf z}_{l-1}^1, {\bf z}_{l-1}^2, \cdots, {\bf z}_{l-1}^9\) to become ${\bf z}_l^0$.</p> <p>The input ${\bf z}_0$ and the output ${\bf z}_L$ have the same dimension, and ${\bf z}_0^i$ in ${\bf z}_0$ eventually becomes ${\bf z}_L^i$ in ${\bf z}_L$</p> <h3 id="classification-tasks">Classification Tasks</h3> <p>Recall that \({\bf x}_{class}\) is the top row of ${\bf z}_0$ denoted by ${\bf z}_0^0$. The patch embedding in ${\bf z}_L$ corresponding to \({\bf x}_{class}\) is its top row ${\bf z}_L^0$.</p> <p>In Eq (4) of <a href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al.</a>, ${\bf z}_L^0$ is used for classification tasks. Alternatively, “mean-pooling” of the patch embeddings \(\frac{1}{10}\sum_{i=0}^{9} {\bf z}_L^i\) can be used.</p> <center width="100%"><img src="/assets/img/vit/classification_on_cls.png" width="200px"/></center> <p>In order to classify the image ${\bf x}$, we compute the logits as</p> \[{\bf o} = {\bf z}_L^0 {\bf W} + {\bf b}\] <ul> <li>${\bf z}_L^0$ is a $1 \times D$ vector.</li> <li>\({\bf W}\) is a $D \times N_\mbox{class}$ matrix,</li> <li>\({\bf b}\) is a $1 \times N_\mbox{class}$ vector.</li> </ul> <p>Then, we determine the softmax</p> \[\mbox{Softmax}({\bf o}) = \left[ \begin{matrix} p_1 \\ p_2 \\ p_3 \\ \vdots \\ p_{N_\mbox{class}} \end{matrix} \right] = \left[ \begin{matrix} P(\mbox{bird}) \\ P(\mbox{ball}) \\ P(\mbox{car}) \\ \vdots \\ P(\mbox{zebra}) \end{matrix} \right]\] <p>In <a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py">Phill Wang’s implementation</a>,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># x == z_0
</span>  <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (batch_size, 10, D)
</span>  <span class="c1"># x == z_L
</span>  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">pool</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span> <span class="k">else</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="ai"/><category term="transformer"/><summary type="html"><![CDATA[ViT notes]]></summary></entry></feed>